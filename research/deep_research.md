# State-of-the-Art Computationally Efficient No-Reference Video Quality Assessment for AI-Generated Content

## Executive Summary
The rapid evolution of AI-driven video generation, exemplified by models such as LTX Studio and WAN, has opened new frontiers in content creation. However, this advancement introduces a critical need for robust No-Reference Video Quality Assessment (NR-VQA) to identify and mitigate novel generative artifacts, including anatomical distortions, unnatural motion, and temporal inconsistencies. A significant challenge lies in developing solutions that operate within stringent low-latency, high-throughput pipelines, targeting a Quality Control (QC) latency of substantially less than 0.25 seconds per 60-second video equivalent.
This report surveys state-of-the-art NR-VQA methodologies tailored for AI-generated video (AGV) quality. Key approaches include specialized metrics like the AI-Generated Human activity Video Quality metric (GHVQ) for human-centric content, advanced CLIP-based models such as COVER and AIGV-Assessor for comprehensive quality evaluation, and optimized optical flow algorithms like RAFT variants and CompactFlowNet for precise temporal artifact detection. Crucially, efficient frame sampling strategies, including Adaptive Keyframe Sampling (AKS) and fragment sampling (e.g., FasterVQA), are indispensable for meeting the demanding computational efficiency requirements. The analysis underscores the pivotal role of high-performance hardware, particularly NVIDIA H100 GPUs, in enabling these computationally intensive tasks within the specified latency constraints. The findings suggest that a multi-faceted approach, combining specialized metrics with intelligent sampling and powerful hardware, is essential for maintaining quality in dynamic AI video production workflows.

## 1. Introduction: The Critical Need for NR-VQA in AI-Generated Video Production

### 1.1 The Landscape of AI-Generated Video and Emerging Quality Challenges
The landscape of AI-driven video generation has experienced transformative progress in recent years. Advanced models, including platforms like LTX Studio and WAN, are now capable of producing dynamic and visually compelling content, significantly impacting sectors such as entertainment, art, and advertising. This technological leap has streamlined content creation workflows, offering unprecedented creative possibilities.1

Despite these impressive capabilities, AI-generated videos (AGVs) frequently exhibit distinct visual and semantic distortions that pose unique quality assessment challenges. Unlike artifacts found in traditionally compressed or transmitted videos, these "generative artifacts" stem from the inherent stochasticity and instability of the generative process itself, often hindering the practical application of AGVs in real-world scenarios.1 Common artifacts include distorted, incomplete, or abnormal human body parts and actions, such as aberrant limb counts, irrational object shapes, physically implausible motion, flickering effects, morphing issues, and inconsistencies in fine details over time.1 A particularly challenging aspect is the struggle of current AI models to maintain character consistency in appearance and attributes, as well as overall motion coherence across video sequences. These temporal inconsistencies can severely detract from the overall perceived quality and usability of the generated content.4

The fundamental difference between these generative artifacts and traditional distortions creates a significant gap in existing quality assessment methodologies. Conventional video quality assessment (VQA) models were primarily developed to evaluate "reconstruction" quality against a pristine reference (Full-Reference or Reduced-Reference VQA) or to detect common compression-induced distortions in natural user-generated content (No-Reference VQA). These models were not designed to identify or accurately quantify the unique visual and semantic inconsistencies inherent in AI-generated media. Consequently, a direct application of existing NR-VQA tools to AGVs often yields suboptimal or misleading quality assessments.11 This highlights a critical need for specialized datasets and metrics that are specifically tailored to reflect human perception of these novel generative distortions, moving beyond traditional signal fidelity measures to capture the complex semantic and temporal coherence issues in AGVs.

### 1.2 Principles of No-Reference Video Quality Assessment (NR-VQA)
No-Reference Video Quality Assessment (NR-VQA), often termed Blind Video Quality Assessment (BVQA), is an essential technique for evaluating the perceptual quality of a video clip without requiring access to its original, pristine reference version.12 This characteristic is paramount for assessing AI-generated content, as a true "original" reference video often does not exist in the generative process, making full-reference methods impractical.

NR-VQA models function by extracting perceptually relevant features directly from the distorted video itself. These features are then mapped to a predicted quality score, typically learned through training on large datasets annotated with human subjective ratings, often expressed as Mean Opinion Scores (MOS).12 The field of NR-VQA has undergone significant evolution. Early approaches relied on hand-crafted features and statistical models inspired by the human visual system. However, recent advancements are overwhelmingly dominated by deep learning-based methods. These models leverage large-scale datasets and powerful neural network architectures to automatically learn and capture complex distortion phenomena and high-level semantic content, leading to superior performance in predicting human perception of quality.12

### 1.3 The Imperative for High-Throughput, Low-Latency QC Pipelines
The user's explicit requirement for a Quality Control (QC) latency significantly less than 0.25 seconds per 60-second original video equivalent presents an exceptionally demanding computational challenge. This translates to an average processing time of approximately 0.004 seconds per second of video, or roughly 0.13 milliseconds per frame for a 30 frames-per-second (FPS) video. Such a stringent target necessitates exceptionally computationally efficient NR-VQA solutions.

In a high-throughput AI video generation pipeline, real-time or near-real-time quality assessment is not merely an advantage but an indispensable requirement. It enables rapid iterative refinement of generative models, automated filtering of low-quality outputs, and ensures consistent quality at scale, thereby preventing bottlenecks in the overall content creation process. Achieving this sub-0.25 second latency for a 60-second video is a formidable computational hurdle. State-of-the-art deep learning models, while offering high accuracy, often come with substantial computational overhead, typically prioritizing predictive power over raw speed.12 This implies that a brute-force application of the most accurate, but computationally heavy, models to every frame of a 60-second video is not feasible within the given latency constraints. For instance, a fast model like COVER reports 79.37 milliseconds to process a 30-frame 4K video clip on an NVIDIA A100 GPU.24 This translates to approximately 2.64 milliseconds per frame. To process a 60-second video at 30 FPS (1800 frames), this would require approximately 4.75 seconds, which is far beyond the 0.25-second target. Similarly, RAFT, a prominent optical flow model, processes 1088x436 videos at 10 FPS on a 1080Ti GPU, equating to 100 milliseconds per frame, with a smaller version achieving 20 FPS (50 milliseconds per frame).25 These figures underscore that even highly optimized models, when applied densely, cannot meet the specified real-time constraints.

Therefore, the QC pipeline must intelligently balance model complexity, input data reduction (e.g., through efficient sampling strategies), and maximal utilization of high-performance computing hardware. This stringent latency requirement dictates a design philosophy where efficiency is a primary driver. It pushes towards the adoption of "lightweight" or "efficient" model variants, even if they offer slightly lower peak accuracy than their larger counterparts, provided they meet the critical latency threshold. This constraint also necessitates innovations in model architecture (e.g., smaller parameter counts, optimized layers), data processing (e.g., intelligent frame sampling), and maximal utilization of powerful GPUs like the NVIDIA A100 and H100.

## 2. State-of-the-Art NR-VQA Methods for AI-Generated Content

### 2.1 GHVQ Metric: Specialized Assessment for Human Activity AGVs
The AI-Generated Human activity Video Quality metric (GHVQ) represents a pioneering objective evaluation metric specifically developed to address the unique quality assessment challenges of AI-generated videos (AGVs) that depict human activities.1 Traditional general image/video quality assessment (I/VQA) metrics and common text-to-video (T2V) metrics, such as Inception Score (IS) and Fréchet Inception Distance (FID), have demonstrated poor performance when evaluating AGVs, particularly due to the inherent difficulties AI models face in generating realistic human figures and actions.1

GHVQ employs a comprehensive and explainable approach by systematically extracting features across multiple dimensions. Its architecture includes:
*   Spatial Quality Analyzer: This module extracts both human-focused and holistic quality features at the frame level. For human-focused aspects, it utilizes body-part segment masks to explicitly extract features specific to individual human body parts. It further refines these features through an inner-body distortion analysis module, which identifies issues within a single body part, and a cross-body distortion analysis module, which assesses interactions and coherence between different body parts. Holistic quality features are extracted using a pre-trained AIGC IQA method to better capture complex AI-generated content artifacts across the entire frame.1
*   Action Quality Analyzer: This component is crucial for evaluating the temporal continuity and realism of human activities within AGVs. It achieves this by employing a pre-trained action recognition model, which helps detect unnatural or discontinuous movements.1
*   Text Feature Extractor: To ensure semantic alignment with the input prompt, a text feature extractor, leveraging models like CLIP, is used to capture semantic features from the text prompts. This helps assess how well the generated video adheres to its textual description and whether semantic issues are present.1
*   Quality Regressor: A multi-layer perceptron (MLP) serves as the quality regressor. This component integrates the extracted spatial quality, action quality, and text features to predict overall quality scores and identify specific semantic distortions in human body parts.1

Extensive experimental results demonstrate that GHVQ significantly outperforms existing quality metrics on the Human-AGVQA dataset by a large margin, showcasing its effectiveness in accurately assessing the quality of complex human activity AGVs.1 The Human-AGVQA dataset itself is a substantial benchmark resource, consisting of 6,000 AGVs generated by 15 popular text-to-video (T2V) models using 400 diverse text prompts describing human activities. It includes rich subjective quality labels for human appearance quality, action continuity quality, and overall video quality, along with expert annotations for semantic artifacts concerning six human body parts.1 This dataset and the GHVQ metric are publicly available.1

### 2.2 CLIP-Based Video Coherence & Quality Metrics
CLIP (Contrastive Language-Image Pre-training) has emerged as a powerful tool in vision-language tasks, and its success extends to video quality assessment for AI-generated content. CLIP-based methods are particularly valuable for evaluating text-video alignment and temporal consistency in AGVs.2

Several specific CLIP-based metrics are employed:
*   CLIP-T: This metric calculates the average cosine similarity between each edited frame and the corresponding textual prompt, assessing how well the visual content aligns with the given text description.28
*   CLIP-F(Tmp-Con): This refers to the average cosine similarity between consecutive edited frames, directly measuring temporal consistency within the video.28
*   LPIPS-P (Learned Perceptual Image Patch Similarity - Perceptual) and LPIPS-T (Temporal): These metrics quantify perceptual deviation. LPIPS-P measures deviation from original video frames, while LPIPS-T measures perceptual differences between adjacent edited frames, thereby assessing temporal consistency.28
*   CLIP-IQA: This metric, often used in benchmarks, measures image quality based on CLIP features.38

Despite their utility, these metrics face limitations, including potential misalignment with human subjective perceptions and an incomplete evaluation due to their focus on single dimensions.28 To address these shortcomings, more advanced CLIP-based models have been developed:
*   COVER: This Comprehensive Video Quality Evaluator is designed for User-Generated Content (UGC) VQA tasks, which often include AI-generated elements. It features three distinct branches: a Semantic Branch (using a CLIP image encoder for semantic quality), an Aesthetic Branch, and a Technical Branch. The semantic branch "gates" the others, facilitating feature interactions. COVER demonstrates high efficiency, achieving a runtime of 79.37 milliseconds for a 30-frame 4K resolution (3840x2160) video clip on an NVIDIA A100 GPU.24 While not explicitly detailed for AGVs, its comprehensive nature suggests applicability. The code for COVER is available on GitHub:(https://github.com/taco-group/COVER).24
*   AIGV-Assessor: This novel VQA model is specifically designed for AGVs, leveraging spatiotemporal features and Large Multimodal Model (LMM) frameworks. It addresses unique distortions in AGVs, such as unrealistic objects, unnatural movements, and inconsistent visual elements. AIGV-Assessor is benchmarked on AIGVQA-DB, a large-scale dataset comprising 36,576 AGVs with 370,000 expert ratings, covering aspects like static quality, temporal smoothness, dynamic degree, and text-video correspondence.2 The dataset and code for AIGV-Assessor are publicly available, with the code found on GitHub: https://github.com/wangjiarui153/AIGV-Assessor.40
*   Face Consistency Benchmark (FCB): This framework specifically evaluates and compares the consistency of characters in AI-generated videos, a critical aspect of realism. FCB utilizes established face recognition models (e.g., VGG-Face, Facenet, ArcFace) and measures consistency using the cosine distance of facial embeddings. It operates in two modes: comparing all frames to a representative frame and comparing random pairs of frames. Benchmarks on models like Runway Gen-3, HunyuanVideo, Vchitect-2.0, and CogVideoX1.5-5B reveal a significant gap between the facial consistency of real videos and AI-generated ones.9

Open-source implementations for CLIP-based and related video quality assessment include VE-Bench, a benchmark suite tailored for text-driven video editing quality assessment, with its code available at(https://github.com/littlespray/VE-Bench) and datasets at(https://openi.pcl.ac.cn/OpenDatasets).28

### 2.3 Optical Flow for Temporal Artifact Detection
Optical flow, which quantifies the pixel-level motion vectors between consecutive frames, is an indispensable tool for understanding motion dynamics in videos.44 Its application is crucial for detecting temporal artifacts prevalent in AI-generated videos, such as jitter, unnatural motion, frozen areas, and general inconsistencies in movement.

Optical flow enables the detection of these artifacts by providing a dense representation of motion. Abnormal score dynamics within diffusion models during video generation can indicate emerging artifacts.5 Temporal attention layers in transformer-based models, which are increasingly used in optical flow estimation, are designed to capture long-range temporal relationships, aiding in identifying motion irregularities.53 Furthermore, discriminators trained on optical flow can assess the realism of generated motion, guiding the generation process towards more natural temporal coherence.45 Techniques like motion compensation and refinement, which rely on accurate optical flow, are also employed to correct inconsistencies and improve frame transitions.49 The integration of optical flow loss functions in generative models helps ensure temporal coherence and motion consistency.46

For efficient optical flow estimation, several algorithms are prominent:
*   RAFT (Recurrent All-Pairs Field Transforms): RAFT is a state-of-the-art deep learning-based model for optical flow estimation, known for its accuracy. While powerful, it can be computationally intensive.25 To address efficiency, a raft_small() model builder is available, offering faster inference at a slight sacrifice in accuracy.47 Further advancements include Ef-RAFT, which achieves significant accuracy improvements (10% on Sintel, 5% on KITTI) over RAFT with only a modest 33% reduction in speed and 13% increase in memory usage.48 MS-RAFT+ is another variant designed for high-resolution, multi-scale optical flow estimation.26 The original RAFT code is available on GitHub:(https://github.com/princeton-vl/RAFT).54
*   CompactFlowNet: This model is designed as a real-time mobile neural network for optical flow prediction. It offers faster inference speeds, with reported latencies such as 118 milliseconds for 512x512 resolution inputs.22 While still too slow for the most aggressive per-frame targets, it represents a step towards more efficient solutions.
*   NVIDIA Optical Flow SDK: This SDK leverages dedicated hardware capabilities on NVIDIA Turing, Ampere, and Ada architecture GPUs to accelerate optical flow computation. It offloads intensive flow vector calculations, significantly reducing computational complexity and freeing up CPU and GPU cycles for other tasks. For instance, it can reduce GPU utilization by up to 80% for object tracking compared to popular software algorithms without compromising accuracy.60 This hardware-accelerated approach is critical for achieving the low-latency requirements on platforms equipped with NVIDIA A10 or H100 GPUs.

Regarding performance on high-end GPUs, the NVIDIA H100 GPU offers substantial advantages over its predecessor, the A100. The H100 is reported to be 2-3 times faster for most workloads, with up to 6 times faster AI training and 30 times faster AI inference speeds for large language models compared to the A100.23 This superior performance, coupled with recent price reductions making H100 cloud pricing more competitive, positions the H100 as a critical component for meeting the demanding computational requirements of real-time NR-VQA in high-throughput pipelines.23 The need for GPU acceleration is evident, as even simple optical flow examples are noted to be "a lot faster" on a GPU, with full video processing being impractical without it.47

### 2.4 Other Lightweight NR-VQA Models for AI Video Artifacts
The unique nature of AI-generated video artifacts necessitates specialized NR-VQA models. Traditional VQA models, primarily designed for "reconstruction" quality, struggle to generalize to the diverse and "in-the-wild" distortions introduced by generative AI.11 Deep learning approaches are crucial for capturing the complex and diverse distortion phenomena, as well as high-level semantic content, aligning quality assessment with human visual perception.14

To meet the stringent low-latency requirements, the focus shifts to computationally efficient and lightweight deep learning architectures:
*   FAST-VQA and FasterVQA: These are end-to-end video quality assessment toolboxes that employ innovative fragment sampling strategies. FasterVQA is an advanced version, offering a remarkable 4x speed improvement over FAST-VQA while maintaining comparable performance.21 These models utilize Grid Mini-patch Sampling (GMS) to efficiently capture both local texture-related quality information and global contextual relations by sampling mini-patches at their raw resolution and covering global quality uniformly.21 They also incorporate a Fragment Attention Network (FANet) and Adaptive Multi-scale Inference (AMI) for robust and flexible performance across various resolutions. FasterVQA is reported to operate faster than real-time, with mobile-friendly variants capable of inference in less than one second on an Apple M1 CPU.16 The code is available on GitHub:(https://github.com/VQAssessment/FAST-VQA-and-FasterVQA).21
*   RAPIQUE: This model combines low-level scene statistics features with high-level semantics-aware deep convolutional features. It employs aggressive spatial and temporal sampling strategies to achieve efficiency and accuracy in quality assessment.12
*   2BiVQA: This metric utilizes a pre-trained Convolutional Neural Network (CNN) for extracting discriminative features from image patches, which are then fed into two Bi-directional Long Short-Term Memory (Bi-LSTM) networks for spatial and temporal pooling. This architecture enables it to achieve high performance at a lower computational cost compared to many state-of-the-art VQA models.16 The source code for 2BiVQA is publicly available on GitHub:(https://github.com/atelili/2BiVQA).16
*   Light-VQA+: Specialized for video exposure correction, Light-VQA+ integrates the CLIP model and vision-language guidance during feature extraction, coupled with a module referencing the Human Visual System (HVS) for more accurate assessment. This model is noted for its efficiency in its specific domain.18
*   PTM-VQA: This method leverages knowledge from models pre-trained on various tasks, using them as feature extractors to predict video quality based on integrated features. PTM-VQA achieves state-of-the-art performance on several NR-VQA datasets with a relatively small number of learnable weights.15
*   AIGC-VQA: This model introduces a general perception metric for assessing the quality of AI-generated content (AIGC) videos across technical, aesthetic, and video-text alignment aspects. It employs a multi-branch architecture, utilizing a 3D-Swin Transformer for technical quality, ConvNext for aesthetic evaluation, and a BLIP-based branch (enhanced with a spatial-temporal adapter) for video-text alignment.65

These lightweight models and architectural innovations are critical for detecting various AI video artifacts:
*   Flickering: Temporal consistency models, such as those employing Long Short-Term Memory (LSTM) networks and Temporal Transformers, are used to eliminate frame flickering and ensure smooth transitions.18
*   Unnatural Motion and Physics: Optical flow is fundamental for motion tracking and detecting unnatural movements.56 Datasets like Physics-IQ are specifically designed to test a video generative model's understanding of physical laws, aiding in the development of metrics for physically plausible motion.68
*   Detail Inconsistency: While not always explicitly named as a metric, the ability to capture high-frequency details and maintain consistency across frames is an inherent goal of many advanced VQA models, particularly those using multi-resolution inputs or detailed feature extraction.15
*   Morphing: CLIP-based approaches have shown utility in morphing attack detection, which can be adapted to identify unwanted morphing artifacts in AI-generated content.26

### 2.5 Human Figure & Face Quality Assessment in AI Video
AI models frequently struggle with generating anatomically accurate human figures, realistic facial synthesis, expressive gestures, and maintaining consistent appearance and attributes over time.1 These challenges manifest as issues like extra or missing limbs, deformed extremities, fused body parts, and unnatural facial expressions.

Specialized metrics and frameworks have been developed to address these specific human-centric quality issues:
*   GHVQ (AI-Generated Human activity Video Quality metric): As discussed in Section 2.1, GHVQ is specifically designed for human activity AGVs. Its human-focused features, body-part segmentation, and inner/cross-body distortion analysis modules are crucial for identifying anatomical inaccuracies and evaluating human appearance quality.1
*   AGHI-Assessor: This Large Multimodal Model (LMM)-based Image Quality Assessment (IQA) method is tailored for AI-generated human images (AGHIs). It predicts perceptual quality scores, text-image correspondence scores, and identifies visible and distorted human body parts (face, body, arm, hand, leg, foot). AGHI-Assessor has demonstrated state-of-the-art performance in multidimensional quality assessment and in detecting structural distortions in AGHIs, surpassing existing IQA methods and leading LMMs.7
*   Face Consistency Benchmark (FCB): This framework directly measures character facial consistency over time in AI-generated videos. It employs various face recognition models (e.g., VGG-Face, Facenet, ArcFace) and uses cosine distance of facial embeddings to quantify consistency. FCB's benchmarks on current generative models highlight a significant disparity in facial consistency compared to real videos, indicating a key area for improvement in AI video generation.9
*   Motion Realism Metrics:
    *   Fréchet Video Distance (FVD): This metric measures how closely generated videos resemble real videos, with lower FVD scores indicating higher video quality and more natural motion.66
    *   Gesture Expressiveness (HKV Metric), Hand Keypoint Confidence (HKC), and Lip-Sync Accuracy: These specialized metrics are used to quantify the naturalness and precision of human gestures, hand movements, and lip synchronization in AI-generated videos.73
*   Physics-IQ Dataset: This dataset is specifically designed to test a video generative model's understanding of physical laws (e.g., collisions, object continuity, gravity). It provides a benchmark for assessing the physical plausibility of motions in AGVs.68

Optical flow, as discussed, is also crucial for assessing and guiding realistic motion generation.44

The development of these specialized metrics is supported by dedicated datasets:
*   Human-AGVQA: Focuses on human activity AGVs, as detailed in Section 2.1.1
*   GAIA (Generic AI-generated Action dataset): Comprises 9,180 AI-generated videos with 971,244 ratings from a large-scale subjective evaluation. It assesses action quality from three causal reasoning-based perspectives: subject quality, action completeness, and action-scene interaction, revealing strengths and weaknesses of T2V models in generating visually rational actions.4
*   AGHI-QA: Consists of 4,000 AI-generated human images with quality-related labels, including perceptual quality, text-image correspondence, and identification of visible and distorted human body parts.7
*   MotionVid: Positioned as the largest human-motion pose dataset, containing approximately 1.2 million data pairs of videos with varying resolutions and durations, designed to facilitate the training of Text-to-Pose tasks for realistic human motion generation.77

### 2.6 Efficient Frame Sampling Strategies for Video QA
Efficient frame sampling strategies are paramount for meeting the stringent low-latency requirements of QC pipelines for AI-generated videos, as processing every frame at full resolution with complex models is computationally prohibitive.16 The goal is to reduce computational burden while preserving critical quality information.

Key sampling strategies include:
*   Uniform Sampling: While simple (e.g., selecting every Nth frame), this method is prone to losing important information, especially in dynamic scenes, and may miss critical artifacts.79
*   Keyframe Detection and Adaptive Sampling: These methods intelligently focus on the most relevant frames based on criteria such as motion detection, scene changes, or content relevance to the input prompt.78
*   Adaptive Keyframe Sampling (AKS): This algorithm aims to maximize useful information with a fixed number of video tokens. It formulates keyframe selection as an optimization problem considering both the relevance between keyframes and the prompt (using a vision-language model) and the coverage of keyframes over the entire video. Experiments demonstrate that AKS can improve video QA accuracy and drastically reduce the number of processed frames, often by more than 90%, by selecting informative keyframes.78
*   Fragment Sampling (FAST-VQA/FasterVQA): As part of the FasterVQA framework, Grid Mini-patch Sampling (GMS) is employed for spatial sampling, while FragmentSampleFrames is used for temporal sampling. This approach samples discontinuous frames or mini-patches to efficiently capture local texture-related quality information while maintaining global context. This allows for efficient end-to-end deep VQA and the learning of effective quality-related representations.16
*   Multi-resolution Input Representation: Some models leverage multi-resolution inputs to preserve high-resolution quality information while reducing the overall computational load. This approach allows for a comprehensive view of both global video composition and local high-resolution details.16
*   Content-Adaptive Encoding/Sampling: Advanced AI algorithms can analyze video content in real-time, making intelligent decisions about compression, resolution scaling, and bandwidth allocation. This means allocating more processing resources or sampling density to action sequences to maintain clarity, while static scenes receive more efficient, sparser processing without sacrificing visual appeal.14

These efficient sampling strategies are critical for achieving the target QC latency of significantly less than 0.25 seconds per 60-second video equivalent. By drastically reducing the amount of data processed per video without compromising the ability to detect critical artifacts, they enable the deployment of sophisticated NR-VQA models in high-throughput, low-latency production environments.

### 2.7 Comparative Analysis & Benchmarks
The evaluation of NR-VQA methods for AI-generated video relies on specialized datasets and a suite of performance metrics. Traditional VQA benchmarks often fall short because they do not capture the unique generative artifacts of AGVs.

Key Datasets for Benchmarking AI-Generated Video Quality:
*   Human-AGVQA: A dataset specifically designed for human activity AGVs, comprising 6,000 videos generated by 15 T2V models using 400 diverse text prompts. It includes subjective quality labels and semantic artifact annotations for human body parts.1
*   GAIA (Generic AI-generated Action dataset): This large-scale dataset focuses on action quality in AGVs, containing 9,180 videos from 18 T2V models with 971,244 human ratings based on causal reasoning perspectives (subject quality, action completeness, action-scene interaction).4
*   AIGVQA-DB: A large-scale dataset with 36,576 AGVs generated by 15 T2V models using 1,048 prompts, accompanied by 370,000 expert ratings. It evaluates static quality, temporal smoothness, dynamic degree, and text-video correspondence.6
*   VE-Bench DB: A video quality assessment database specifically for text-driven video editing.28
*   Physics-IQ: Consists of 396 videos covering 66 different physical scenarios, designed to test a video generative model's understanding of physical laws and object behavior.68
*   Face Consistency Benchmark (FCB): Focuses on character facial consistency over time in AGVs, benchmarking models like Runway Gen-3 and HunyuanVideo against real video baselines.9
*   UVE-Bench: A benchmark that collects videos from state-of-the-art Video Generation Models (VGMs) and provides pairwise human preference annotations across 15 evaluation aspects, used to evaluate 16 Large Multimodal Models (MLLMs).2

Traditional UGC VQA Datasets: While not exclusively for AGVs, datasets like LSVQ, KoNViD-1k, LIVE-VQC, and YouTube-UGC are frequently used for benchmarking general NR-VQA models and can provide a baseline for performance.13

Performance Metrics:
*   Accuracy:
    *   Spearman Rank Correlation Coefficient (SRCC) and Pearson Linear Correlation Coefficient (PLCC): These are standard metrics for quantifying the correlation between objective quality scores and human subjective judgments, with higher values indicating better alignment.1
    *   Fréchet Video Distance (FVD) and Inception Score (IS): Commonly used for evaluating the realism and diversity of generated videos. Lower FVD and higher IS generally indicate better quality.30
    *   CLIP-IQA, MUSIQ, DOVER, NRQM: Other metrics used in various benchmarks for image and video quality.38
*   Specialized Metrics: Lip-Sync Accuracy, Gesture Expressiveness (HKV Metric), Hand Keypoint Confidence (HKC) for human motion realism 73; Cosine Distance for facial consistency.9
*   Computational Cost:
    *   MACs (Multiply-Accumulate Operations): A measure of computational complexity, indicating the number of operations required by a model.21
    *   Inference Time (ms) / FPS: Direct measures of processing speed. For example, COVER reports 79.37ms for a 30-frame 4K video on an A100 GPU.24 RAFT processes 1088x436 videos at 10 FPS on a 1080Ti GPU, with a smaller version achieving 20 FPS.25 CompactFlowNet reports 118ms for 512x512 resolution.22
*   Latency: Critical for the user's target of significantly less than 0.25 seconds per 60-second video. This aggressive target necessitates models that can process video segments in milliseconds, which is often achieved through efficient sampling and highly optimized hardware.

Hardware Considerations (NVIDIA A10/H100 GPUs):
The performance benchmarks consistently highlight the necessity of powerful GPUs for efficient video quality assessment. The NVIDIA H100 GPU offers significant advantages over the A100, providing 2-3 times faster performance for most workloads, up to 6 times faster AI training, and a remarkable 30 times faster AI inference for large language models.23 This superior computational capacity, coupled with higher memory bandwidth (3.35 TB/s HBM3 vs. 2 TB/s HBM2e on A100) and native FP8 support, makes the H100 a preferred choice for demanding AI workloads.23 The NVIDIA Optical Flow SDK further optimizes performance by offloading optical flow computation to dedicated hardware engines on Turing, Ampere, and Ada GPUs, significantly reducing GPU utilization for other tasks.60 These hardware advancements are crucial for achieving the low-latency, high-throughput requirements of modern AI video QC pipelines.

## 3. Conclusions & Recommendations
The landscape of AI-generated video content, driven by models like LTX Studio and WAN, presents a compelling need for sophisticated No-Reference Video Quality Assessment. The unique "generative artifacts" and temporal inconsistencies inherent in these videos necessitate specialized metrics and computationally efficient solutions. Achieving the ambitious target QC latency of significantly less than 0.25 seconds per 60-second video equivalent demands a multi-faceted approach that intelligently combines advanced algorithms, efficient data processing strategies, and high-performance hardware.

The analysis indicates that a direct, frame-by-frame application of even fast state-of-the-art VQA models at high resolutions is unlikely to meet the stringent latency target. For instance, models like COVER, while efficient for individual clips, would exceed the 0.25-second threshold for a full 60-second video if applied densely. This highlights that the latency constraint is exceptionally tight and requires strategic implementation.

Recommendations for Practical Implementation:
*   Prioritize Specialized Metrics for AGVs: For human-centric AI-generated video segments, the GHVQ metric is highly recommended. Its ability to systematically analyze human-focused quality features, body-part distortions, and action continuity offers a granular and explainable assessment that traditional metrics lack.1 For broader quality and coherence, AIGV-Assessor is a strong candidate due to its LMM-based framework tailored for AGVs and its comprehensive evaluation across multiple perceptual dimensions.2 For critical character consistency, deploy the principles of the Face Consistency Benchmark (FCB), leveraging established face recognition models to quantify facial coherence over time.9
*   Leverage Efficient Optical Flow for Temporal Artifacts: Optical flow remains indispensable for detecting temporal irregularities. Implement highly optimized optical flow algorithms such as RAFT variants (e.g., raft_small for speed, or Ef-RAFT for a balance of speed and accuracy).47 Crucially, utilize the NVIDIA Optical Flow SDK to offload computation to dedicated hardware on NVIDIA A10/H100 GPUs, significantly boosting processing speed and reducing overall GPU utilization for real-time motion analysis.60 This hardware acceleration is vital for detecting jitter, unnatural motion, and frozen areas.
*   Implement Aggressive and Adaptive Frame Sampling: To meet the low-latency, high-throughput requirements, intelligent frame sampling strategies are non-negotiable. Adopt methods like Adaptive Keyframe Sampling (AKS), which selects the most informative frames based on relevance to the prompt and video coverage, potentially reducing processed frames by over 90%.78 Alternatively, consider the fragment sampling approach used by FasterVQA, which efficiently captures both local details and global context across sparse, discontinuous frames.16 A tiered QC system, where initial rapid checks use highly sparse sampling and more detailed analyses are triggered only for flagged segments, could be considered.
*   Invest in High-Performance GPU Infrastructure: The computational demands of state-of-the-art NR-VQA models, even with efficient sampling, necessitate powerful hardware. Prioritize NVIDIA H100 GPUs over A100s for their superior inference speed (up to 30x faster for LLMs), higher memory bandwidth, and overall performance leadership in AI workloads.23 This investment is critical for enabling the required throughput and minimizing latency in the QC pipeline.
*   Explore Lightweight End-to-End Models: For general AI video quality assessment, consider lightweight end-to-end models like FasterVQA due to its demonstrated efficiency and real-time capabilities.16 Its mobile-friendly variants, capable of inference in less than one second on less powerful CPUs, suggest a highly optimized architecture that could be further leveraged on high-end GPUs.

Future Research and Development Directions:
*   Continued Dataset Development: The field requires more specialized, large-scale datasets with granular annotations for diverse generative artifacts, particularly those related to complex temporal dynamics, physics, and semantic consistency in AGVs.
*   Multi-Modal LMM-based VQA: Further development of Large Multimodal Models (LMMs) for VQA, as seen in AIGV-Assessor, holds promise for more nuanced and human-aligned quality predictions that integrate visual, textual, and potentially audio information.
*   Real-time Physics-Based Motion Assessment: Research into integrating physics-based understanding directly into VQA models, potentially leveraging datasets like Physics-IQ, could lead to more robust detection of unnatural motion and physically implausible events.
*   Adaptive Model Architectures: Continued innovation in model architectures that can dynamically adjust their computational complexity based on video content and real-time latency requirements will be crucial for optimizing the trade-off between accuracy and speed.

By strategically adopting these state-of-the-art methods, prioritizing computational efficiency through intelligent sampling and hardware acceleration, and continuously adapting to emerging generative AI capabilities, it is possible to build a robust and high-performance NR-VQA pipeline capable of ensuring quality in the rapidly evolving landscape of AI-generated video.

## Works cited
Human-Activity AGV Quality Assessment: A Benchmark Dataset and an Objective Evaluation Metric - arXiv, accessed May 23, 2025, https://arxiv.org/html/2411.16619v2
(PDF) UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? - ResearchGate, accessed May 23, 2025, https://www.researchgate.net/publication/389821112_UVE_Are_MLLMs_Unified_Evaluators_for_AI-Generated_Videos
Wan2.1 vs Hunyuan vs LTXV: Which Best Image to Video AI Tool - MimicPC, accessed May 23, 2025, https://www.mimicpc.com/learn/wan-vs-hunyuan-vs-ltxv-which-best-image-to-video-ai-tool
GAIA: Rethinking Action Quality Assessment for AI-Generated Videos, accessed May 23, 2025, https://proceedings.neurips.cc/paper_files/paper/2024/file/46b5405a720a99db4c758cff43c8b4d3-Paper-Datasets_and_Benchmarks_Track.pdf
Temporal Score Analysis for Understanding and Correcting Diffusion Artifacts - arXiv, accessed May 23, 2025, https://arxiv.org/html/2503.16218
AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM - arXiv, accessed May 23, 2025, https://arxiv.org/html/2411.17221v1
AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images - arXiv, accessed May 23, 2025, https://arxiv.org/html/2504.21308v1
Evaluating and Predicting Distorted Human Body Parts for Generated Images - arXiv, accessed May 23, 2025, https://arxiv.org/html/2503.00811v1
Face Consistency Benchmark for GenAI Video - arXiv, accessed May 23, 2025, https://arxiv.org/html/2505.11425v1
DirectorLLM for Human-Centric Video Generation - arXiv, accessed May 23, 2025, https://arxiv.org/html/2412.14484v1
Quality Prediction of AI Generated Images and Videos: Emerging Trends and Opportunities, accessed May 23, 2025, https://www.researchgate.net/publication/384886970_Quality_Prediction_of_AI_Generated_Images_and_Videos_Emerging_Trends_and_Opportunities
RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated Content, accessed May 23, 2025, https://live.ece.utexas.edu/publications/2021/RAPIQUE.pdf
A Deep Learning based No-reference Quality Assessment Model for UGC Videos - arXiv, accessed May 23, 2025, https://arxiv.org/pdf/2204.14047
Video Quality Assessment: A Comprehensive Survey - arXiv, accessed May 23, 2025, https://arxiv.org/html/2412.04508v2
PTM-VQA: Efficient Video Quality Assessment Leveraging Diverse PreTrained Models from the Wild - CVF Open Access, accessed May 23, 2025, https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_PTM-VQA_Efficient_Video_Quality_Assessment_Leveraging_Diverse_PreTrained_Models_from_CVPR_2024_paper.pdf
ADS-VQA: Adaptive sampling model for video quality assessment - ResearchGate, accessed May 23, 2025, https://www.researchgate.net/publication/382006522_ADS-VQA_Adaptive_sampling_model_for_video_quality_assessment?_tp=eyJjb250ZXh0Ijp7InBhZ2UiOiJzY2llbnRpZmljQ29udHJpYnV0aW9ucyIsInByZXZpb3VzUGFnZSI6bnVsbCwic3ViUGFnZSI6bnVsbH19
Perceptual video quality assessment: the journey continues! - Frontiers, accessed May 23, 2025, https://www.frontiersin.org/journals/signal-processing/articles/10.3389/frsip.2023.1193523/full
Light-VQA+: A Video Quality Assessment Model for Exposure Correction with Vision-Language Guidance - arXiv, accessed May 23, 2025, https://arxiv.org/html/2405.03333v2
No-Reference Video Quality Assessment Using Multi-Pooled, Saliency Weighted Deep Features and Decision Fusion - PubMed, accessed May 23, 2025, https://pubmed.ncbi.nlm.nih.gov/35336380/
No-reference video quality assessment for user generated content based on deep network and visual perception - ResearchGate, accessed May 23, 2025, https://www.researchgate.net/publication/355409567_No-reference_video_quality_assessment_for_user_generated_content_based_on_deep_network_and_visual_perception
VQAssessment/FAST-VQA-and-FasterVQA: [ECCV2022 ... - GitHub, accessed May 23, 2025, https://github.com/VQAssessment/FAST-VQA-and-FasterVQA
CompactFlowNet: Efficient Real-time Optical Flow Estimation on Mobile Devices - arXiv, accessed May 23, 2025, https://arxiv.org/html/2412.13273v1
NVIDIA H100 vs A100: Detailed GPU Comparison for 2024 | Jarvislabs.ai Docs, accessed May 23, 2025, https://docs.jarvislabs.ai/blog/h100vsa100
taco-group/COVER: [CVPRW 2024] COVER: A ... - GitHub, accessed May 23, 2025, https://github.com/taco-group/COVER
arXiv:2003.12039v3 [cs.CV] 25 Aug 2020 - Pequan, accessed May 23, 2025, https://www-pequan.lip6.fr/~bereziat/cours/master/vision/papers/raft2020.pdf
arxiv.org, accessed May 23, 2025, https://arxiv.org/abs/2411.16619
[Literature Review] Human-Activity AGV Quality Assessment: A Benchmark Dataset and an Objective Evaluation Metric - Moonlight, accessed May 23, 2025, https://www.themoonlight.io/en/review/human-activity-agv-quality-assessment-a-benchmark-dataset-and-an-objective-evaluation-metric
ojs.aaai.org, accessed May 23, 2025, https://ojs.aaai.org/index.php/AAAI/article/view/32763/34918
Daily Papers - Hugging Face, accessed May 23, 2025, https://huggingface.co/papers?q=video%20motion%20quality
Measuring What Matters: Objective Metrics for Image Generation Assessment, accessed May 23, 2025, https://huggingface.co/blog/PrunaAI/objective-metrics-for-image-generation-assessment
ICLR 2025 Spotlights, accessed May 23, 2025, https://iclr.cc/virtual/2025/events/spotlight-posters
Al Generated Morph Cut Transition? : r/premiere - Reddit, accessed May 23, 2025, https://www.reddit.com/r/premiere/comments/1kb82a3/al_generated_morph_cut_transition/
[2504.21334] Simple Visual Artifact Detection in Sora-Generated Videos - arXiv, accessed May 23, 2025, https://arxiv.org/abs/2504.21334
FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation, accessed May 23, 2025, https://arxiv.org/html/2504.07405v1
Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model, accessed May 23, 2025, https://www.researchgate.net/publication/385135978_Empowering_Morphing_Attack_Detection_using_Interpretable_Image-Text_Foundation_Model
Generating Automatically Print/Scan Textures for Morphing Attack Detection Applications This work is supported by the European Union's Horizon 2020 research and innovation program under grant agreement No 883356 and by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center - arXiv, accessed May 23, 2025, https://arxiv.org/html/2408.09558v1
Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment - arXiv, accessed May 23, 2025, https://arxiv.org/html/2403.11956v1
DiffVSR: Revealing an Effective Recipe for Taming Robust Video Super-Resolution Against Complex Degradations - arXiv, accessed May 23, 2025, https://arxiv.org/html/2501.10110v3
AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM - ResearchGate, accessed May 23, 2025, https://www.researchgate.net/publication/386143665_AIGV-Assessor_Benchmarking_and_Evaluating_the_Perceptual_Quality_of_Text-to-Video_Generation_with_LMM
[2411.17221] AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM - arXiv, accessed May 23, 2025, https://arxiv.org/abs/2411.17221
wangjiarui153 - GitHub, accessed May 23, 2025, https://github.com/wangjiarui153
[2505.11425] Face Consistency Benchmark for GenAI Video - arXiv, accessed May 23, 2025, https://arxiv.org/abs/2505.11425
littlespray/VE-Bench: [AAAI 25] Official Implementation for ... - GitHub, accessed May 23, 2025, https://github.com/littlespray/VE-Bench
OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models - arXiv, accessed May 23, 2025, https://arxiv.org/html/2411.10501v1
Optical-Flow Guided Prompt Optimization for Coherent Video Generation - arXiv, accessed May 23, 2025, https://arxiv.org/html/2411.15540v1
Enhanced cell tracking using a GAN-based super-resolution video-to-video time-lapse microscopy generative model - PubMed Central, accessed May 23, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11994914/
Optical Flow: Predicting movement with the RAFT model — Torchvision 0.12 documentation, accessed May 23, 2025, https://docs.pytorch.org/vision/0.12/auto_examples/plot_optical_flow.html
Rethinking RAFT for Efficient Optical Flow - arXiv, accessed May 23, 2025, https://arxiv.org/html/2401.00833v1
AI-DRIVEN REAL-TIME VIDEO ENHANCEMENT USING SUPER-RESOLUTION AND OPTICAL FLOW TECHNIQUE - ICTACT Journals, accessed May 23, 2025, http://ictactjournals.in/paper/IJIVP_Vol_15_Iss_3_Paper_9_3523_3528.pdf
Video Object Detection: Definition, Algorithms, and Best Practices [2023] - Kili Technology, accessed May 23, 2025, https://kili-technology.com/data-labeling/computer-vision/video-annotation/what-is-video-object-detection-definition-and-best-practices
Optical Flow Estimation - Department of Computer Science, University of Toronto, accessed May 23, 2025, https://www.cs.toronto.edu/~fleet/research/Papers/flowChapter05.pdf
Optical Flow - Everything You Need to Know - viso.ai, accessed May 23, 2025, https://viso.ai/deep-learning/optical-flow/
Optical Flow as Spatial-Temporal Attention Learners, accessed May 23, 2025, https://www.computer.org/csdl/journal/tp/2024/12/10705000/20JUjMtDQVa
RAFT - The Spring Dataset & Benchmark, accessed May 23, 2025, https://spring-benchmark.org/52/
opticalFlowRAFT - Estimate optical flow using RAFT deep learning algorithm - MATLAB, accessed May 23, 2025, https://www.mathworks.com/help/vision/ref/opticalflowraft.html
Enhancing Real-Time Video Processing With Artificial Intelligence: Overcoming Resolution Loss, Motion Artifacts, And Temporal Inconsistencies | Journal of Information Systems Engineering and Management, accessed May 23, 2025, https://jisem-journal.com/index.php/journal/article/view/6540
Enhancing Real-Time Video Processing With Artificial Intelligence: Overcoming Resolution Loss, Motion Artifacts, And Temporal Inconsistencies - ResearchGate, accessed May 23, 2025, https://www.researchgate.net/publication/391232537_Enhancing_Real-Time_Video_Processing_With_Artificial_Intelligence_Overcoming_Resolution_Loss_Motion_Artifacts_And_Temporal_Inconsistencies
FacialFlowNet: Advancing Facial Optical Flow Estimation with a Diverse Dataset and a Decomposed Model - arXiv, accessed May 23, 2025, https://arxiv.org/html/2409.05396v1
MS-RAFT+ - Spring benchmark, accessed May 23, 2025, https://spring-benchmark.org/51/
Optical Flow SDK - NVIDIA Developer, accessed May 23, 2025, https://developer.nvidia.com/optical-flow-sdk
NVIDIA GPUs: H100 vs. A100 | a detailed comparison - Gcore, accessed May 23, 2025, https://gcore.com/blog/nvidia-h100-a100
NVIDIA H100 GPU Performance Shatters Machine Learning Benchmarks For Model Training - Moor Insights & Strategy, accessed May 23, 2025, https://moorinsightsstrategy.com/nvidia-h100-gpu-performance-shatters-machine-learning-benchmarks-for-model-training/
Video Quality Assessment on MSU NR VQA Database - Papers With Code, accessed May 23, 2025, https://paperswithcode.com/sota/video-quality-assessment-on-msu-video-quality
GitHub - vztu/BVQA_Benchmark: A resource list and performance benchmark for blind video quality assessment (BVQA) models on user-generated content (UGC) datasets. [IEEE TIP'2021] "UGC-VQA, accessed May 23, 2025, https://github.com/vztu/BVQA_Benchmark
AIGC-VQA: A Holistic Perception Metric for AIGC Video Quality Assessment - ResearchGate, accessed May 23, 2025, https://www.researchgate.net/publication/384417639_AIGC-VQA_A_Holistic_Perception_Metric_for_AIGC_Video_Quality_Assessment
FloVD: Optical Flow Meets Video Diffusion Model for Enhanced Camera-Controlled Video Synthesis - arXiv, accessed May 23, 2025, https://arxiv.org/html/2502.08244v1
FloVD: Optical Flow Meets Video Diffusion Model for Enhanced Camera-Controlled Video Synthesis - arXiv, accessed May 23, 2025, https://arxiv.org/html/2502.08244v2
\titlefontDo generative video models learn physical principles from watching videos? - arXiv, accessed May 23, 2025, https://arxiv.org/html/2501.09038v1
Blind Video Quality Assessment for Ultra-High-Definition Video Based on Super-Resolution and Deep Reinforcement Learning - MDPI, accessed May 23, 2025, https://www.mdpi.com/1424-8220/23/3/1511
3D Digital Human Generation from a Single Image Using Generative AI with Real-Time Motion Synchronization - MDPI, accessed May 23, 2025, https://www.mdpi.com/2079-9292/14/4/777
Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions - arXiv, accessed May 23, 2025, https://arxiv.org/html/2504.19056v1
Consistent Human Image and Video Generation with Spatially Conditioned Diffusion - arXiv, accessed May 23, 2025, https://arxiv.org/html/2412.14531v1
ByteDance OmniHuman-1: New method for realistic video generation - Tech Pilot, accessed May 23, 2025, https://techpilot.ai/bytedance-omnihuman-1-human-video-generator/
NeurIPS Poster GAIA: Rethinking Action Quality Assessment for AI-Generated Videos, accessed May 23, 2025, https://neurips.cc/virtual/2024/poster/97817
GAIA: Rethinking Action Quality Assessment for AI-Generated Videos - OpenReview, accessed May 23, 2025, https://openreview.net/forum?id=BZe6dmDk5K&referrer=%5Bthe%20profile%20of%20Guangtao%20Zhai%5D(%2Fprofile%3Fid%3D~Guangtao_Zhai1)
GAIA: Rethinking Action Quality Assessment for AI-Generated Videos - arXiv, accessed May 23, 2025, https://arxiv.org/html/2406.06087v2
HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation, accessed May 23, 2025, https://arxiv.org/html/2503.24026v1
Adaptive Video Understanding Agent: Enhancing efficiency with dynamic frame sampling and feedback-driven reasoning | PromptLayer, accessed May 23, 2025, https://www.promptlayer.com/research-papers/this-ai-agent-understands-videos-by-watching-only-key-frames
Adaptive Keyframe Sampling for Long Video Understanding - arXiv, accessed May 23, 2025, https://arxiv.org/html/2502.21271v1
What are best practices for frame sampling and selection? - Milvus, accessed May 23, 2025, https://milvus.io/ai-quick-reference/what-are-best-practices-for-frame-sampling-and-selection
Azure AI Video Indexer (VI) scene shot keyframe detection insight overview | Microsoft Learn, accessed May 23, 2025, https://learn.microsoft.com/en-us/azure/azure-video-indexer/scene-shot-keyframe-detection-insight
AI Video Quality Enhancement: 6 Breakthrough Features for Perfect Streaming - Fora Soft, accessed May 23, 2025, https://www.forasoft.com/blog/article/ai-video-quality-enhancement
</final_file_content>

IMPORTANT: For any future changes to this file, use the final_file_content shown above as your reference. This content reflects the current state of the file, including any auto-formatting (e.g., if you used single quotes but the formatter converted them to double quotes). Always base your SEARCH/REPLACE operations on this final version to ensure accuracy.

<environment_details>
# VSCode Visible Files
research/deep_research.md

# VSCode Open Tabs
research/o3_plan.md
research/README_4.md
research/search_results copy.md
research/deep_research_prompt.md
research/deep_research.md

# Current Time
5/23/2025, 6:27:07 PM (America/Vancouver, UTC-7:00)

# Context Window Usage
729,714 / 1,048.576K tokens used (70%)

# Current Mode
ACT MODE
</environment_details>
