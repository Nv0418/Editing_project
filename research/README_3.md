# README3.md: Adapting Movis for VinVideo

Hello! This guide explains how we can take the existing Movis video editing library and tailor it to become the powerful editing engine behind our new VinVideo platform.

## What Movis Does Today: A Quick Recap

Movis is a clever Python library that lets developers create and edit videos using code. Instead of clicking around in a traditional video editor, you write scripts to:

*   Build video scenes (called "Compositions").
*   Add layers like video clips, images, text, and shapes.
*   Animate these layers – make them move, change size, fade in and out.
*   Apply visual effects like blurs or color adjustments.
*   Mix audio and generate subtitles.

It's great for tasks that need precision, automation, or complex effects that are easier to define with logic than by hand.

## The VinVideo Editing Interface: What's New?

While Movis is powerful for coders, VinVideo needs an editing system that fits into our automated, AI-driven pipeline and is also usable for review and tweaks. Here’s how it will work:

1.  **AI-Powered Editing Plans**: Our "Editor Agent" (a smart AI) will look at the original scene prompt from the user, plus the creative vision from our "Director Agent" and "DoP Agent." Based on all this, it will create a detailed "edit plan" – like a recipe for the video – probably in a structured format like JSON or XML.
2.  **Movis as the Engine**: Movis will take this edit plan and do the actual video assembly and rendering.
3.  **User Preview & Tweaks**: Users (and our internal teams) will need to see what the Editor Agent has planned and be able to make small adjustments – things like changing text, slightly altering the timing of an effect, or swapping an image.
4.  **Smart Re-makes**: If a part of the raw video (generated by our other AIs) isn't quite right, the Editor Agent (or a user) can flag it. Our system then needs to tell the *original* video/image generation AIs to remake just that specific piece, and then Movis will use the new piece in the edit.
5.  **(Future Idea) Basic Quality Check**: We might add a simple automated step to scan for any really obvious visual glitches in the video.

**The Big Difference**: Instead of someone writing Movis Python code for each video, Movis will become the "hands" that execute the edit plan created by our AI Editor. We'll also need a user-friendly way to peek at and adjust these AI-generated edits.

## Customizing Movis for VinVideo: Our Development Plan

To integrate Movis as VinVideo's core editing engine, we'll develop the following key components and enhancements:

1.  **VinVideo Interactive Editing Interface**
    *   **Our Goal**: Provide a user-friendly way for our team and eventually users to preview the Editor Agent's output, inspect layers, and make targeted adjustments (e.g., text edits, timing tweaks, asset swaps).
    *   **Our Approach with Movis**:
        *   Since Movis is a library, we will build a dedicated VinVideo preview and light-editing tool. This could start as an internal web-based application or a specialized desktop tool.
        *   This tool, likely housed in a new `vinvideo/editing_interface/` module within our extended Movis project, will parse the Editor Agent's structured edit plan.
        *   It will leverage Movis to render previews (still frames or short clips) on demand.
        *   When a user makes a tweak, the interface will modify the structured edit plan, which then triggers a re-preview via Movis.
        *   We will extend Movis's `Composition` class (`movis/layer/composition.py`) or create wrappers to easily expose its layer structure and properties for programmatic access and modification by our interface.

2.  **VinVideo Editor Agent Connector**
    *   **Our Goal**: Seamlessly translate the Editor Agent's JSON/XML edit plans into executable Movis operations.
    *   **Our Approach with Movis**:
        *   We will develop a robust `vinvideo/agent_connector/plan_parser.py` module.
        *   This parser will map elements from the edit plan (e.g., "add_text_layer", "apply_transition_effect_X") to specific Movis API calls (e.g., `movis.layer.Text(...)`, `layer.add_effect(...)`, `movis.ops.fade_in(...)`).
        *   This connector will dynamically generate and execute the Movis script corresponding to the AI's vision, making Movis the execution engine for our AI-driven editing.

3.  **VinVideo Asset Regeneration Pipeline**
    *   **Our Goal**: Enable efficient re-creation of only flawed raw video/image segments by our upstream AI generators, and ensure Movis uses these updated assets.
    *   **Our Approach with Movis**:
        *   The Editor Agent's edit plan must assign unique identifiers to all source assets.
        *   Our VinVideo platform will manage the workflow: if an asset is flagged (either by QA or a user), it will trigger the appropriate upstream AI generator with the original prompt/parameters for that specific asset.
        *   Once a new version of the asset is available, its entry in the edit plan will be updated. Movis will then naturally pick up the new asset path/ID when it next processes that plan.
        *   To facilitate this, we will enhance media layers in `movis/layer/media.py` to optionally store and expose these VinVideo asset identifiers, linking them directly to the Movis layer.

4.  **Integrated QA Sampling & Analysis Pipeline**
    *   **The Need for QA**: While Movis itself is a deterministic rendering engine, the final video's visual quality in VinVideo depends on many factors: the AI-generated source assets, the complexity of the programmatic edits, and the potential for unintended interactions in procedural content. Therefore, an automated QA process is crucial to catch visual defects early.
    *   **Our Goal**: Implement an automated first-pass quality check to identify and flag significant visual issues in rendered video segments, thereby improving overall output quality and streamlining the review process.
    *   **Our Approach with Movis & VinVideo Services**:
        *   **Frame Extraction with Movis**: Movis's `Composition` object (`movis/layer/composition.py`) allows for the extraction of specific frames from any point in the timeline. We will develop a `vinvideo/qa_pipeline/sampler.py` module. After Movis renders a video segment as per the Editor Agent's plan, this sampler will be triggered. It will use intelligent heuristics (e.g., sampling at regular intervals, around edit points, during high-motion sequences, or based on metadata from the Editor Agent) to extract a representative set of frames.
        *   **VinVideo's Internal AI-Powered QC Service**: We will build and deploy a dedicated VinVideo Quality Control (QC) service. This service will feature an API for receiving image frames and returning analysis results. The `sampler.py` will dispatch the extracted frames to this internal service.
        *   **Automated Defect Detection**: Our QC service will employ a suite of AI models and image analysis algorithms to detect a range of predefined visual issues, including but not limited to:
            *   **Defect Categories**: Our QC service will employ a suite of AI models and image analysis algorithms to detect a range of predefined visual issues. These fall into several categories:
                *   *Upstream AI Asset Artifacts*: Issues originating from the initial AI video/image generation, such as distorted features (e.g., hands, faces), unnatural textures, object inconsistencies (e.g., an object morphing or disappearing), flickering, or nonsensical details.
                *   *Movis Rendering & Composition Issues*: Problems that might arise during Movis's assembly, such as missing layers (if an asset path was broken), incorrect colors (if a color transformation was misapplied), text illegibility due to poor contrast or placement, elements unexpectedly overlapping due to animation errors, or issues from complex procedural effects generated by Movis itself if their parameters are not well-tuned.
                *   *Technical & Broadcast Standards*: Issues like excessive blur, severe color banding, black/empty frames, significant aliasing, interlacing artifacts (if applicable), or audio-video sync problems (though primary A/V sync is handled by Movis's timeline).
            The service will return structured data, such as quality scores, lists of detected defect types, confidence levels, and potentially coordinates or heatmaps of problem areas.
        *   **Actionable Feedback Loop & MCP Integration**: The QA results are critical for our closed-loop system:
            *   *Automated Asset Regeneration*: High-severity defects, especially those identified as upstream AI asset artifacts, will automatically trigger the "VinVideo Asset Regeneration Pipeline" (detailed in point 3). Our system will request the original AI generator to remake the flawed asset, possibly with varied seeds or parameters.
            *   *Editor Agent Adaptation*: The Editor Agent will receive QA feedback. It can use this to:
                *   Modify its editing plan if a Movis-driven effect or composition choice led to a problem (e.g., simplify a procedural transition, change text placement).
                *   Specifically request regeneration of a source asset if it's deemed the cause.
                *   If multiple regeneration attempts fail, flag the segment for human review.
            *   *Human Review Queue*: Segments with persistent issues or medium-severity defects will be routed to a human review interface.
            *   *MCP Tooling*: Key functionalities of this QA pipeline (e.g., "submit_segment_for_qa", "get_qa_report", "trigger_asset_regeneration_due_to_qa") will be exposed as new **MCP tools**. This empowers our AI agents to actively participate in and manage the quality control lifecycle.
            This integrated, proactive QA step is vital for maintaining a high standard of visual quality across all VinVideo outputs and for making our automated pipeline more robust and efficient.

## VinVideo Pre-Editing Asset QA: Ensuring Quality at the Source

A critical stage for quality control in VinVideo is immediately after our upstream AI models generate the initial video/image segments, and *before* these assets are passed to the Editor Agent for assembly and further editing in Movis. This pre-editing QA ensures that the Editor Agent works with the highest quality source material, minimizing downstream issues and regeneration loops.

*   **Our Goal**: To implement a robust, multi-stage QA process. This involves:
    1.  Quality checking initial source *images* if our pipeline uses an image-to-video approach.
    2.  Quality checking the AI-generated *video segments* (from LTX Studio, WAN model, etc.) *before* they are passed to the Editor Agent and Movis.
*   **Workflow**:
    1.  **Source Image Generation & QC (Conditional)**:
        *   This step applies if our content strategy involves generating still images first, which then serve as inputs for an image-to-video model.
        *   An upstream AI generates a source image.
        *   This image is immediately submitted to our **QWEN-2.5 VL model** (or a similar specialized image QC model we deploy within VinVideo). This QC focuses on image-specific artifacts: clarity, composition, object coherence, and alignment with the initial prompt.
        *   **If Failed**: The image is rejected. The image generation AI is triggered to re-create it, potentially with varied parameters or seeds.
        *   **If Passed**: The approved image is now a qualified input for the next stage.
    2.  **Video Segment Generation**: Our primary video generation AIs (e.g., **LTX Studio model, WAN model**) produce a raw video segment. This can be through direct text-to-video synthesis or by using pre-vetted source images (from step 1) in an image-to-video process.
    3.  **Raw Video Segment QA Submission**: Each newly generated video segment is automatically submitted to "VinVideo's Internal AI-Powered QC Service." This service will employ appropriate models for video analysis (which could include frame-by-frame analysis using models like QWEN-VL, or specialized video QC models we develop or integrate).
    4.  **Focused Defect Detection in Video Segments**: The QC service analyzes the video segment for:
        *   *Temporal Coherence*: Flickering, inconsistent motion, unnatural speed changes within the segment.
        *   *AI Video Artifacts*: Morphing, object/scene distortions, poor transitions if the segment is meant to be continuous, lack of detail, or other common issues from video generation models.
        *   *Alignment*: Consistency with the source prompt and/or the input images used.
    5.  **Efficient Sampling Strategy for Video Segment QA**: To manage processing load and latency for video segments:
        *   *Smart Frame Sampling*: We will analyze a strategic subset of frames (e.g., 1-2 frames per second, keyframes identified by content change, frames at the start/middle/end of the segment) rather than every single frame by default.
        *   *Content-Adaptive Analysis*: Basic motion analysis or scene complexity metrics *within* the raw segment can guide the QC service to focus more intensively on visually complex or dynamic portions.
        *   *Optimized QC Models*: Our internal video QC models will be selected and optimized for both speed and accuracy in identifying relevant defects in AI-generated video content.
    6.  **Pass/Fail Decision & Video Regeneration Loop**:
        *   The QC service provides a clear pass/fail decision for each raw video segment.
        *   **If Failed**: The segment is rejected. Our VinVideo platform automatically triggers the responsible upstream video generation AI (LTX Studio, WAN model) to re-create that specific segment. This regeneration request can include feedback from the QC (e.g., "excessive flickering detected") to guide the next attempt, along with varied seeds or parameters. This loop continues for a defined number of attempts.
        *   **If Passed**: The video segment is approved and cataloged in our asset library, ready for the Editor Agent to incorporate into its Movis edit plan.
*   **Benefits of this Staged Pre-Editing QA**:
    *   **Improved Source Quality**: By rigorously QC'ing source images (if used) and then the raw video segments, we ensure that only high-quality material enters the creative editing stage. This directly addresses the "garbage in, garbage out" problem.
    *   **Efficient Resource Use**: The Editor Agent and Movis don't waste processing cycles on fundamentally flawed assets. Regeneration happens at the source, which is more efficient.
    *   **Targeted Post-Editing QA**: The subsequent QA step (point 4 in the "Customizing Movis for VinVideo" section, which occurs *after* Movis editing) can then focus on issues related to the edit itself—composition, timing, effect quality, narrative coherence—rather than being bogged down by raw asset generation flaws.

This comprehensive, multi-stage pre-editing QA strategy is foundational for ensuring the consistent quality and reliability of the VinVideo platform.

## Next Steps

Here’s a suggested path forward to integrate Movis into VinVideo:

1.  **Scaffold the Editor Agent Connector**: Start by building the basic translator that can take a simple version of the AI Editor's plan and make Movis create a video from it.
2.  **Design Minimal Editing Preview/UI**: Figure out the simplest way users can see the AI-edited video and request small changes (even if it's just editing the plan textually at first).
3.  **Expose Regeneration Hooks**: Ensure our edit plan and system can identify and manage segments that need to be re-created by the upstream AIs, and that Movis can use these updated assets.
4.  **Integrate QA Sampling**: Implement the basic frame sampling and connect it to an initial image Quality Control model.
5.  **Document and Test Each Feature**: As each piece is built, make sure it's well-documented and tested so the whole VinVideo pipeline is robust.

By extending Movis in these ways, we can create a very flexible and powerful editing layer for VinVideo, combining AI-driven automation with the ability for human oversight and creativity.
